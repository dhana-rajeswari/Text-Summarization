{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9d8dc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.55.4)\n",
      "Requirement already satisfied: datasets in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch transformers datasets sentencepiece pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f99aa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "RAW_DIR = \"./raw_data\"\n",
    "PROCESSED_DIR = \"./processed\"\n",
    "\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "682d9c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "CALLS_FILE = os.path.join(RAW_DIR, r\"C:\\Users\\saisr\\Downloads\\archive\\call_recordings.csv\")\n",
    "df = pd.read_csv(CALLS_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e52f5d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pseudo_summary(row):\n",
    "    parts = []\n",
    "    if \"Type\" in row and pd.notna(row[\"Type\"]):\n",
    "        parts.append(f\"This was a {row['Type'].lower()} call\")\n",
    "    if \"Sentiment\" in row and pd.notna(row[\"Sentiment\"]):\n",
    "        parts.append(f\"with a {row['Sentiment'].lower()} sentiment\")\n",
    "    if \"Name\" in row and pd.notna(row[\"Name\"]):\n",
    "        parts.append(f\"involving {row['Name']}\")\n",
    "    if \"Order Number\" in row and pd.notna(row[\"Order Number\"]):\n",
    "        parts.append(f\"regarding order {row['Order Number']}\")\n",
    "    return \", \".join(parts) if parts else \"Customer service call summary.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fcfbd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for _, r in df.iterrows():\n",
    "    src = r.get(\"Transcript\", \"\")\n",
    "    tgt = make_pseudo_summary(r)\n",
    "    records.append({\"src\": src, \"tgt\": tgt})\n",
    "\n",
    "out_df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be607a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 16 train and 4 val samples in ./processed\n"
     ]
    }
   ],
   "source": [
    "train_df = out_df.sample(frac=0.8, random_state=42)\n",
    "val_df = out_df.drop(train_df.index)\n",
    "\n",
    "train_df.to_csv(os.path.join(PROCESSED_DIR, \"train.csv\"), index=False)\n",
    "val_df.to_csv(os.path.join(PROCESSED_DIR, \"val.csv\"), index=False)\n",
    "\n",
    "print(f\"âœ… Saved {len(train_df)} train and {len(val_df)} val samples in {PROCESSED_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b264d4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.10.0)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: transformers in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.55.4)\n",
      "Requirement already satisfied: torch in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from accelerate) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from accelerate) (0.34.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Downloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "Installing collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.10.0\n",
      "    Uninstalling accelerate-1.10.0:\n",
      "      Successfully uninstalled accelerate-1.10.0\n",
      "Successfully installed accelerate-1.10.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade accelerate transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf1d496d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "print(accelerate.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec177dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] transformers version detected: 4.55.4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4609bc7d4a10413d9d81cee52ed7e7ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb660ca7c684f20bf9196e8e263f24c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] TrainingArguments being used:\n",
      "  - output_dir = ./bart_finetuned\n",
      "  - per_device_train_batch_size = 2\n",
      "  - per_device_eval_batch_size = 2\n",
      "  - learning_rate = 5e-05\n",
      "  - num_train_epochs = 3\n",
      "  - weight_decay = 0.01\n",
      "  - logging_dir = ./logs\n",
      "  - logging_steps = 10\n",
      "  - save_total_limit = 2\n",
      "  - save_strategy = epoch\n",
      "  - load_best_model_at_end = False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saisr\\AppData\\Local\\Temp\\ipykernel_13212\\2501540153.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\saisr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24/24 02:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>11.041000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>7.436700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saisr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\modeling_utils.py:3922: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "c:\\Users\\saisr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\saisr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fine-tuned model saved to ./bart_finetuned\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Fine-tune BART on your call transcripts with maximum compatibility\n",
    "across different transformers versions (old/new).\n",
    "\n",
    "Requires:\n",
    "  - processed/train.csv  (columns: src, tgt)\n",
    "  - processed/val.csv    (columns: src, tgt)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import inspect\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    BartTokenizerFast,\n",
    "    BartForConditionalGeneration,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    __version__ as TRANSFORMERS_VERSION,\n",
    ")\n",
    "\n",
    "print(f\"[info] transformers version detected: {TRANSFORMERS_VERSION}\")\n",
    "\n",
    "# --------------------------\n",
    "# Paths\n",
    "# --------------------------\n",
    "TRAIN_CSV = \"./processed/train.csv\"\n",
    "VAL_CSV   = \"./processed/val.csv\"\n",
    "OUT_DIR   = \"./bart_finetuned\"\n",
    "\n",
    "assert os.path.exists(TRAIN_CSV), f\"Missing {TRAIN_CSV}. Run your Step-1 script first.\"\n",
    "assert os.path.exists(VAL_CSV),   f\"Missing {VAL_CSV}. Run your Step-1 script first.\"\n",
    "\n",
    "# --------------------------\n",
    "# Load data\n",
    "# --------------------------\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "val_df   = pd.read_csv(VAL_CSV)\n",
    "\n",
    "# Ensure required columns\n",
    "for dfname, df in [(\"train\", train_df), (\"val\", val_df)]:\n",
    "    for col in (\"src\", \"tgt\"):\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"[error] {dfname}.csv must contain column '{col}'\")\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "val_ds   = Dataset.from_pandas(val_df)\n",
    "\n",
    "# --------------------------\n",
    "# Load tokenizer & model\n",
    "# --------------------------\n",
    "MODEL_NAME = \"facebook/bart-base\"\n",
    "tokenizer = BartTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "model     = BartForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# --------------------------\n",
    "# Tokenization (compatible)\n",
    "# --------------------------\n",
    "MAX_IN, MAX_OUT = 512, 128\n",
    "\n",
    "def preprocess(batch):\n",
    "    # Encode inputs\n",
    "    model_inputs = tokenizer(\n",
    "        batch[\"src\"],\n",
    "        max_length=MAX_IN,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Encode targets; try the new API first, fallback to old\n",
    "    try:\n",
    "        labels = tokenizer(\n",
    "            text_target=batch[\"tgt\"],\n",
    "            max_length=MAX_OUT,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "    except TypeError:\n",
    "        # Older transformers: use as_target_tokenizer()\n",
    "        with tokenizer.as_target_tokenizer():  # type: ignore[attr-defined]\n",
    "            labels = tokenizer(\n",
    "                batch[\"tgt\"],\n",
    "                max_length=MAX_OUT,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "            )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "train_ds = train_ds.map(preprocess, batched=True, remove_columns=[\"src\", \"tgt\"])\n",
    "val_ds   = val_ds.map(preprocess,   batched=True, remove_columns=[\"src\", \"tgt\"])\n",
    "\n",
    "# --------------------------\n",
    "# Build TrainingArguments dynamically\n",
    "# --------------------------\n",
    "sig = inspect.signature(TrainingArguments.__init__)\n",
    "allowed = set(sig.parameters.keys()) - {\"self\"}\n",
    "\n",
    "def add_if_supported(d, key, value):\n",
    "    if key in allowed:\n",
    "        d[key] = value\n",
    "\n",
    "args_dict = {}\n",
    "# Required\n",
    "add_if_supported(args_dict, \"output_dir\", OUT_DIR)\n",
    "\n",
    "# Batch size: prefer per_device*, fallback to per_gpu*\n",
    "if \"per_device_train_batch_size\" in allowed:\n",
    "    args_dict[\"per_device_train_batch_size\"] = 2\n",
    "    args_dict[\"per_device_eval_batch_size\"]  = 2 if \"per_device_eval_batch_size\" in allowed else None\n",
    "else:\n",
    "    # Very old versions\n",
    "    add_if_supported(args_dict, \"per_gpu_train_batch_size\", 2)\n",
    "    add_if_supported(args_dict, \"per_gpu_eval_batch_size\", 2)\n",
    "\n",
    "# Common safe args\n",
    "add_if_supported(args_dict, \"learning_rate\", 5e-5)\n",
    "add_if_supported(args_dict, \"num_train_epochs\", 3)\n",
    "add_if_supported(args_dict, \"weight_decay\", 0.01)\n",
    "add_if_supported(args_dict, \"logging_dir\", \"./logs\")\n",
    "add_if_supported(args_dict, \"logging_steps\", 10)\n",
    "add_if_supported(args_dict, \"save_total_limit\", 2)\n",
    "\n",
    "# fp16 only if GPU and arg exists\n",
    "if torch.cuda.is_available():\n",
    "    add_if_supported(args_dict, \"fp16\", True)\n",
    "\n",
    "# Evaluation/save strategies (newer versions only)\n",
    "# If your version is old, these simply won't be set.\n",
    "add_if_supported(args_dict, \"evaluation_strategy\", \"epoch\")\n",
    "add_if_supported(args_dict, \"save_strategy\", \"epoch\")\n",
    "add_if_supported(args_dict, \"load_best_model_at_end\", False)  # optional; keep False to avoid extra requirements\n",
    "\n",
    "# Remove any None values (in case per_device_eval_batch_size was missing)\n",
    "args_dict = {k: v for k, v in args_dict.items() if v is not None}\n",
    "\n",
    "print(\"[info] TrainingArguments being used:\")\n",
    "for k, v in args_dict.items():\n",
    "    print(f\"  - {k} = {v}\")\n",
    "\n",
    "training_args = TrainingArguments(**args_dict)\n",
    "\n",
    "# --------------------------\n",
    "# Custom Trainer that also saves tokenizer\n",
    "# --------------------------\n",
    "class TrainerWithTokenizer(Trainer):\n",
    "    def save_model(self, output_dir=None, _internal_call=False):\n",
    "        super().save_model(output_dir, _internal_call)\n",
    "        if self.tokenizer is not None:\n",
    "            self.tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# --------------------------\n",
    "# Trainer\n",
    "# --------------------------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds if \"evaluation_strategy\" in args_dict or \"evaluate_during_training\" in args_dict else val_ds,  # safe to pass anyway\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Train & Save\n",
    "# --------------------------\n",
    "trainer.train()\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "trainer.save_model(OUT_DIR)\n",
    "tokenizer.save_pretrained(OUT_DIR)\n",
    "print(f\"âœ… Fine-tuned model saved to {OUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8dc2b4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Summary: This was a call, with a question, involving David Wilson. \"Hi, this was a concern call, regarding a wireless complaint.â€\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartTokenizerFast\n",
    "\n",
    "# Load your fine-tuned model\n",
    "MODEL_PATH = \"./bart_finetuned\"\n",
    "tokenizer = BartTokenizerFast.from_pretrained(MODEL_PATH)\n",
    "model = BartForConditionalGeneration.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Example transcript (replace with your call transcript)\n",
    "call_transcript = \"\"\"\n",
    "Hi, this is David Wilson. I'm looking at the SPK-9988 speaker system online, and I'm a little confused. The description says it's wireless, but then it also mentions needing a power cord. Can you explain that? Does it need to be plugged in all the time, or does it have a rechargeable battery? Also, what is the range on the wireless connection?\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(\n",
    "    [call_transcript],\n",
    "    max_length=512,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Generate summary\n",
    "with torch.no_grad():\n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=128,\n",
    "        num_beams=4,\n",
    "        length_penalty=2.0,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(\"ðŸ”¹ Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cdbb2871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soundfile in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.13.1)\n",
      "Collecting librosa\n",
      "  Downloading librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from soundfile) (2.2.6)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (0.61.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (5.2.1)\n",
      "Collecting pooch>=1.1 (from librosa)\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Downloading soxr-0.5.0.post1-cp312-abi3-win_amd64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (4.14.0)\n",
      "Collecting lazy_loader>=0.1 (from librosa)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting msgpack>=1.0 (from librosa)\n",
      "  Downloading msgpack-1.1.1-cp313-cp313-win_amd64.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: standard-aifc in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (3.13.0)\n",
      "Collecting standard-sunau (from librosa)\n",
      "  Downloading standard_sunau-3.13.0-py3-none-any.whl.metadata (914 bytes)\n",
      "Requirement already satisfied: pycparser in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: packaging in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from lazy_loader>=0.1->librosa) (24.2)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pooch>=1.1->librosa) (4.3.8)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.4.26)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn>=1.1.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: standard-chunk in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from standard-aifc->librosa) (3.13.0)\n",
      "Requirement already satisfied: audioop-lts in c:\\users\\saisr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from standard-aifc->librosa) (0.2.1)\n",
      "Downloading librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "Downloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading msgpack-1.1.1-cp313-cp313-win_amd64.whl (72 kB)\n",
      "Downloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Downloading soxr-0.5.0.post1-cp312-abi3-win_amd64.whl (164 kB)\n",
      "Downloading standard_sunau-3.13.0-py3-none-any.whl (7.4 kB)\n",
      "Installing collected packages: standard-sunau, soxr, msgpack, lazy_loader, audioread, pooch, librosa\n",
      "\n",
      "   ---------------------------- ----------- 5/7 [pooch]\n",
      "   ---------------------------------------- 7/7 [librosa]\n",
      "\n",
      "Successfully installed audioread-3.0.1 lazy_loader-0.4 librosa-0.11.0 msgpack-1.1.1 pooch-1.8.2 soxr-0.5.0.post1 standard-sunau-3.13.0\n"
     ]
    }
   ],
   "source": [
    "! pip install soundfile librosa\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0d1af4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TRANSCRIPT ---\n",
      "\n",
      " Hey, I want to order that new game console, the game console X, and an extra game controller Y. This is Mike Johnson. You guys got any deals on those right now? And how fast can you ship it to me? I'm hoping to get it by the weekend.\n",
      "\n",
      "--- SUMMARY ---\n",
      "\n",
      "This was a order call, with a sentiment, involving Mike Johnson. This was a call, involving Gary Johnson, regarding orders X and Y. ï¿½ï¿½\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from transformers import BartForConditionalGeneration, BartTokenizerFast, pipeline\n",
    "\n",
    "# --------------------------\n",
    "# Load Whisper ASR\n",
    "# --------------------------\n",
    "asr = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-small\")\n",
    "\n",
    "# --------------------------\n",
    "# Load Fine-Tuned BART\n",
    "# --------------------------\n",
    "MODEL_PATH = \"./bart_finetuned\"  # your saved model folder\n",
    "tokenizer = BartTokenizerFast.from_pretrained(MODEL_PATH)\n",
    "model = BartForConditionalGeneration.from_pretrained(MODEL_PATH)\n",
    "\n",
    "def summarize_audio(audio_path: str):\n",
    "\n",
    "    audio, sr = sf.read(audio_path)   # tensor, [channels, time]\n",
    "    if sr != 16000:\n",
    "        audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)\n",
    "        sr = 16000\n",
    "        \n",
    "    # 1. Transcribe audio\n",
    "    transcript = asr({\"array\": audio, \"sampling_rate\": sr})[\"text\"]\n",
    "\n",
    "    # 2. Summarize transcript with fine-tuned BART\n",
    "    inputs = tokenizer(transcript, max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        summary_ids = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=128,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return transcript, summary\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: pass your audio file path\n",
    "    audio_file = \"C:\\\\Users\\\\saisr\\\\Downloads\\\\archive\\\\call_recording_14.wav\"  # change this to your file\n",
    "    transcript, summary = summarize_audio(audio_file)\n",
    "\n",
    "    print(\"\\n--- TRANSCRIPT ---\\n\")\n",
    "    print(transcript)\n",
    "\n",
    "    print(\"\\n--- SUMMARY ---\\n\")\n",
    "    print(summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
