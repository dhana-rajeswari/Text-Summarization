{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9d8dc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.55.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.0)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.1-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dhana\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dhana\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dhana\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dhana\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading scikit_learn-1.7.1-cp313-cp313-win_amd64.whl (8.7 MB)\n",
      "   ---------------------------------------- 0.0/8.7 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 4.2/8.7 MB 26.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.7/8.7 MB 28.6 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   ---------------------------------------- 2/2 [scikit-learn]\n",
      "\n",
      "Successfully installed scikit-learn-1.7.1 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torch transformers datasets sentencepiece pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f99aa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "RAW_DIR = \"./raw_data\"\n",
    "PROCESSED_DIR = \"./processed\"\n",
    "\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "682d9c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "CALLS_FILE = os.path.join(RAW_DIR, r\"C:\\Users\\dhana\\Downloads\\pro\\call_recordings.csv\")\n",
    "df = pd.read_csv(CALLS_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e52f5d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pseudo_summary(row):\n",
    "    parts = []\n",
    "    if \"Type\" in row and pd.notna(row[\"Type\"]):\n",
    "        parts.append(f\"This was a {row['Type'].lower()} call\")\n",
    "    if \"Sentiment\" in row and pd.notna(row[\"Sentiment\"]):\n",
    "        parts.append(f\"with a {row['Sentiment'].lower()} sentiment\")\n",
    "    if \"Name\" in row and pd.notna(row[\"Name\"]):\n",
    "        parts.append(f\"involving {row['Name']}\")\n",
    "    if \"Order Number\" in row and pd.notna(row[\"Order Number\"]):\n",
    "        parts.append(f\"regarding order {row['Order Number']}\")\n",
    "    return \", \".join(parts) if parts else \"Customer service call summary.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fcfbd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for _, r in df.iterrows():\n",
    "    src = r.get(\"Transcript\", \"\")\n",
    "    tgt = make_pseudo_summary(r)\n",
    "    records.append({\"src\": src, \"tgt\": tgt})\n",
    "\n",
    "out_df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be607a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 16 train and 4 val samples in ./processed\n"
     ]
    }
   ],
   "source": [
    "train_df = out_df.sample(frac=0.8, random_state=42)\n",
    "val_df = out_df.drop(train_df.index)\n",
    "\n",
    "train_df.to_csv(os.path.join(PROCESSED_DIR, \"train.csv\"), index=False)\n",
    "val_df.to_csv(os.path.join(PROCESSED_DIR, \"val.csv\"), index=False)\n",
    "\n",
    "print(f\"✅ Saved {len(train_df)} train and {len(val_df)} val samples in {PROCESSED_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec177dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] transformers version detected: 4.55.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf8768444fa40e7abe3e0c78760a974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d90b546cff4454180e1e28f644868e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] TrainingArguments being used:\n",
      "  - output_dir = ./bart_finetuned\n",
      "  - per_device_train_batch_size = 2\n",
      "  - per_device_eval_batch_size = 2\n",
      "  - learning_rate = 5e-05\n",
      "  - num_train_epochs = 3\n",
      "  - weight_decay = 0.01\n",
      "  - logging_dir = ./logs\n",
      "  - logging_steps = 10\n",
      "  - save_total_limit = 2\n",
      "  - save_strategy = epoch\n",
      "  - load_best_model_at_end = False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhana\\AppData\\Local\\Temp\\ipykernel_25836\\3069121965.py:148: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\dhana\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24/24 03:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>11.041000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>7.436700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhana\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\modeling_utils.py:3917: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  if is_main_process:\n",
      "c:\\Users\\dhana\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\dhana\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fine-tuned model saved to ./bart_finetuned\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fine-tune BART on your call transcripts with maximum compatibility\n",
    "across different transformers versions (old/new).\n",
    "\n",
    "Requires:\n",
    "  - processed/train.csv  (columns: src, tgt)\n",
    "  - processed/val.csv    (columns: src, tgt)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import inspect\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    BartTokenizerFast,\n",
    "    BartForConditionalGeneration,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    __version__ as TRANSFORMERS_VERSION,\n",
    ")\n",
    "\n",
    "print(f\"[info] transformers version detected: {TRANSFORMERS_VERSION}\")\n",
    "\n",
    "# --------------------------\n",
    "# Paths\n",
    "# --------------------------\n",
    "TRAIN_CSV = \"./processed/train.csv\"\n",
    "VAL_CSV   = \"./processed/val.csv\"\n",
    "OUT_DIR   = \"./bart_finetuned\"\n",
    "\n",
    "assert os.path.exists(TRAIN_CSV), f\"Missing {TRAIN_CSV}. Run your Step-1 script first.\"\n",
    "assert os.path.exists(VAL_CSV),   f\"Missing {VAL_CSV}. Run your Step-1 script first.\"\n",
    "\n",
    "# --------------------------\n",
    "# Load data\n",
    "# --------------------------\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "val_df   = pd.read_csv(VAL_CSV)\n",
    "\n",
    "# Ensure required columns\n",
    "for dfname, df in [(\"train\", train_df), (\"val\", val_df)]:\n",
    "    for col in (\"src\", \"tgt\"):\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"[error] {dfname}.csv must contain column '{col}'\")\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "val_ds   = Dataset.from_pandas(val_df)\n",
    "\n",
    "# --------------------------\n",
    "# Load tokenizer & model\n",
    "# --------------------------\n",
    "MODEL_NAME = \"facebook/bart-base\"\n",
    "tokenizer = BartTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "model     = BartForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# --------------------------\n",
    "# Tokenization (compatible)\n",
    "# --------------------------\n",
    "MAX_IN, MAX_OUT = 512, 128\n",
    "\n",
    "def preprocess(batch):\n",
    "    # Encode inputs\n",
    "    model_inputs = tokenizer(\n",
    "        batch[\"src\"],\n",
    "        max_length=MAX_IN,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Encode targets; try the new API first, fallback to old\n",
    "    try:\n",
    "        labels = tokenizer(\n",
    "            text_target=batch[\"tgt\"],\n",
    "            max_length=MAX_OUT,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "    except TypeError:\n",
    "        # Older transformers: use as_target_tokenizer()\n",
    "        with tokenizer.as_target_tokenizer():  # type: ignore[attr-defined]\n",
    "            labels = tokenizer(\n",
    "                batch[\"tgt\"],\n",
    "                max_length=MAX_OUT,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "            )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "train_ds = train_ds.map(preprocess, batched=True, remove_columns=[\"src\", \"tgt\"])\n",
    "val_ds   = val_ds.map(preprocess,   batched=True, remove_columns=[\"src\", \"tgt\"])\n",
    "\n",
    "# --------------------------\n",
    "# Build TrainingArguments dynamically\n",
    "# --------------------------\n",
    "sig = inspect.signature(TrainingArguments.__init__)\n",
    "allowed = set(sig.parameters.keys()) - {\"self\"}\n",
    "\n",
    "def add_if_supported(d, key, value):\n",
    "    if key in allowed:\n",
    "        d[key] = value\n",
    "\n",
    "args_dict = {}\n",
    "# Required\n",
    "add_if_supported(args_dict, \"output_dir\", OUT_DIR)\n",
    "\n",
    "# Batch size: prefer per_device*, fallback to per_gpu*\n",
    "if \"per_device_train_batch_size\" in allowed:\n",
    "    args_dict[\"per_device_train_batch_size\"] = 2\n",
    "    args_dict[\"per_device_eval_batch_size\"]  = 2 if \"per_device_eval_batch_size\" in allowed else None\n",
    "else:\n",
    "    # Very old versions\n",
    "    add_if_supported(args_dict, \"per_gpu_train_batch_size\", 2)\n",
    "    add_if_supported(args_dict, \"per_gpu_eval_batch_size\", 2)\n",
    "\n",
    "# Common safe args\n",
    "add_if_supported(args_dict, \"learning_rate\", 5e-5)\n",
    "add_if_supported(args_dict, \"num_train_epochs\", 3)\n",
    "add_if_supported(args_dict, \"weight_decay\", 0.01)\n",
    "add_if_supported(args_dict, \"logging_dir\", \"./logs\")\n",
    "add_if_supported(args_dict, \"logging_steps\", 10)\n",
    "add_if_supported(args_dict, \"save_total_limit\", 2)\n",
    "\n",
    "# fp16 only if GPU and arg exists\n",
    "if torch.cuda.is_available():\n",
    "    add_if_supported(args_dict, \"fp16\", True)\n",
    "\n",
    "# Evaluation/save strategies (newer versions only)\n",
    "# If your version is old, these simply won't be set.\n",
    "add_if_supported(args_dict, \"evaluation_strategy\", \"epoch\")\n",
    "add_if_supported(args_dict, \"save_strategy\", \"epoch\")\n",
    "add_if_supported(args_dict, \"load_best_model_at_end\", False)  # optional; keep False to avoid extra requirements\n",
    "\n",
    "# Remove any None values (in case per_device_eval_batch_size was missing)\n",
    "args_dict = {k: v for k, v in args_dict.items() if v is not None}\n",
    "\n",
    "print(\"[info] TrainingArguments being used:\")\n",
    "for k, v in args_dict.items():\n",
    "    print(f\"  - {k} = {v}\")\n",
    "\n",
    "training_args = TrainingArguments(**args_dict)\n",
    "\n",
    "# --------------------------\n",
    "# Trainer\n",
    "# --------------------------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds if \"evaluation_strategy\" in args_dict or \"evaluate_during_training\" in args_dict else val_ds,  # safe to pass anyway\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Train & Save\n",
    "# --------------------------\n",
    "trainer.train()\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "trainer.save_model(OUT_DIR)\n",
    "tokenizer.save_pretrained(OUT_DIR)\n",
    "print(f\"✅ Fine-tuned model saved to {OUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "738ff897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.47.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\dhana\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement sqlite3 (from versions: none)\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for sqlite3\n"
     ]
    }
   ],
   "source": [
    "pip install streamlit pandas sqlite3 pytesseract pillow moviepy SpeechRecognition docx2txt pypdf2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
